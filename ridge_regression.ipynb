{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_str_mp = pyodbc.connect(\"driver={SQL Server};server=main_server\\main_server;database=mp;trusted_connection=true\")\n",
    "orders_xsell = pd.read_sql_query(sql = 'SELECT * FROM [test_db].[sell].[order_sell]', con = con_str_mp)\n",
    "con_str_mp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECAY_WEIGHT = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_xsell(xsell_orders, decay_weight):\n",
    "    main_goods = xsell_orders[['date', 'order_id', 'applestore', 'product_id', 'product_code', \n",
    "                               'product_name', 'category_id', 'category_name', 'subcategory', 'color', \n",
    "                               'im_brand_name']]\n",
    "    for name in main_goods.columns[3:]:\n",
    "         main_goods.rename({name : \"main_\" + name}, axis='columns', inplace = True)\n",
    "    cross_goods = xsell_orders[['order_id', 'product_id', 'product_code', 'product_name', \n",
    "                          'category_id', 'category_name', 'subcategory', 'color', 'im_brand_name',\n",
    "                          'order_qty', 'order_price', 'order_item', 'margin_plus_discount']]  \n",
    "    for name in cross_goods.columns[1:]:\n",
    "         cross_goods.rename({name : \"cross_\" + name}, axis='columns', inplace = True)\n",
    "    xsell = main_goods.merge(cross_goods, on = 'order_id', how = 'inner')\n",
    "     # не рекомендуем к товару его же самого\n",
    "    xsell = xsell[xsell['main_product_code'] != xsell['cross_product_code']].sort_values('date')\n",
    "    xsell['date'] = xsell['date'].apply(lambda x: datetime.strptime(x[:10], '%Y-%m-%d'))\n",
    "    xsell['weight'] = xsell['date'].max() - xsell['date'] \n",
    "    xsell['weight'] = decay_weight ** xsell['weight'].dt.days\n",
    "    xsell['profit'] = xsell['cross_margin_plus_discount']\n",
    "    xsell.sort_values(['main_product_id', 'cross_product_id'], inplace = True)\n",
    "    xsell = pd.concat(\n",
    "                [xsell[['main_product_id',  'cross_product_id', 'order_id'\n",
    "                         ]].groupby(['main_product_id',  'cross_product_id']).count(),\n",
    "                xsell[['main_product_id',  'cross_product_id', 'weight', 'cross_order_item', 'profit'\n",
    "                         ]].groupby(['main_product_id',  'cross_product_id']).sum(),\n",
    "                xsell[['main_product_id',  'cross_product_id','cross_order_price', \n",
    "                       'cross_margin_plus_discount'\n",
    "                        ]].groupby(['main_product_id',  'cross_product_id']).last(), \n",
    "                xsell[['main_product_id',  'cross_product_id', 'applestore', 'main_product_code', \n",
    "                          'main_product_name', 'main_category_name', 'main_subcategory','cross_product_code', \n",
    "                          'cross_product_name','cross_category_name', 'cross_subcategory'\n",
    "                        ]].groupby(['main_product_id',  'cross_product_id']).first()],\n",
    "                      axis = 1, join = 'inner')\n",
    "    xsell.reset_index(inplace = True)\n",
    "    xsell['purchases'] = xsell['weight'] / xsell['weight'].sum()  \n",
    "    xsell.sort_values('main_product_code')\n",
    "    xsell['uniq_orders'] = xsell['order_id']\n",
    "    xsell.rename({'order_id':'poisson_target','weight':'gaussian_target','cross_order_item':'revenue',\n",
    "                  'cross_order_price':'price','cross_margin_plus_discount':'margin', \n",
    "                  'purchases':'binomial_target'}, axis='columns', inplace = True)\n",
    "    return xsell\n",
    "\n",
    "orders_xsell = prepare_xsell(orders_xsell, DECAY_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product_name(text, letters_num = 3):\n",
    "    def stemmer_word(word_list):\n",
    "        out_list = [stemmer.stem(word) for word in word_list]\n",
    "        return out_list\n",
    "    stemmer = SnowballStemmer(\"russian\") \n",
    "    text = (text.str.lower()\n",
    "                .str.replace(\"\\(|\\)\", \" \")\n",
    "                .str.replace(\"\\s[а-яА-Я]{1,\" + str(letters_num) + \"}\\s\", \" \")\n",
    "                .str.replace(\"/\", \" \")\n",
    "                .str.replace(\"[\\s\\t]+\", \" \")\n",
    "                .str.split(\" \")\n",
    "                .apply(stemmer_word)\n",
    "                .str.join(\" \"))\n",
    "    return text\n",
    "\n",
    "orders_xsell['main_product_name'] = clean_product_name(orders_xsell['main_product_name'])\n",
    "orders_xsell['cross_product_name'] = clean_product_name(orders_xsell['cross_product_name'])\n",
    "train_cat_data = orders_xsell[['poisson_target', 'uniq_orders', 'applestore', 'main_product_name', \n",
    "                                    'cross_product_name']]\n",
    "train_cat_data.rename({'main_product_name':'cleaned_main_product',\n",
    "                     'cross_product_name':'cleaned_cross_product'}, axis='columns', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth = 2 \n",
    "def creat_augmented(train_table, growth):\n",
    "    augmented_table = pd.concat([np.repeat(train_table['cleaned_cross_product'], growth)\n",
    "                                 .sample(len(train_table) * growth).reset_index(drop = True),\n",
    "                np.repeat(train_table['cleaned_main_product'], growth).reset_index(drop = True),\n",
    "                np.repeat(train_table['applestore'], growth).reset_index(drop = True)],\n",
    "              axis = 1)\n",
    "    augmented_table['poisson_target'], augmented_table['uniq_orders'] = 0, 1\n",
    "    augmented_table = pd.merge(left = augmented_table,\n",
    "         right = train_table[['cleaned_main_product', 'cleaned_cross_product']],\n",
    "         how = 'left',\n",
    "         indicator = True,\n",
    "        on = ['cleaned_main_product', 'cleaned_cross_product']\n",
    "             )\n",
    "    augmented_table = augmented_table.loc[augmented_table._merge == 'left_only', :].drop(columns='_merge')\n",
    "    augmented_table.drop_duplicates(inplace = True)\n",
    "    augmented_table = pd.concat([train_table, augmented_table], axis = 0, sort = True)\n",
    "    augmented_table = (augmented_table.groupby(['cleaned_cross_product', 'cleaned_main_product', 'applestore'], \n",
    "                                               as_index=False).sum())\n",
    "    augmented_table.sort_values(by='poisson_target',ascending=False, inplace=True)\n",
    "    (augmented_table.drop_duplicates(subset = ['cleaned_main_product', 'cleaned_cross_product', 'applestore'], \n",
    "                                    inplace = True))\n",
    "    return augmented_table.reset_index(drop = True)\n",
    "augmented_train = creat_augmented(train_cat_data, growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictors(df):\n",
    "    table_words = pd.merge(df['cleaned_main_product'].apply(lambda x: x.split()),\n",
    "                           df['cleaned_cross_product'].apply(lambda x: x.split()), how='outer',left_index=True, right_index=True)\n",
    "    function_cartesian = lambda x: itertools.product(x.cleaned_main_product, x.cleaned_cross_product)\n",
    "    table_words['bag_words'] = table_words.apply(function_cartesian, axis=1)\n",
    "    table_words['bag_words'] = table_words['bag_words'].apply(list)\n",
    "    function_join = lambda x: list(map('_'.join, x))\n",
    "    table_words['bag_words'] = table_words['bag_words'].apply(function_join)\n",
    "    vec = HashingVectorizer(ngram_range=(1, 1), n_features=(2 ** 18), norm=None, alternate_sign=True,\n",
    "                            binary=True)\n",
    "\n",
    "    array_words = np.array(table_words['bag_words'].str.join(\" \"))\n",
    "    matrix = vec.fit_transform(array_words)\n",
    "    gc.collect()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = create_predictors(augmented_train)\n",
    "y = augmented_train['poisson_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_regression = linear_model.Ridge(alpha = 10)\n",
    "regression = test_regression.fit(x, y)"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
